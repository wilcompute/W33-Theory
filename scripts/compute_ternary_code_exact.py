"""Enumerate the ternary code generated by point-incidence rows (over GF(3)).
- Computes exact weight enumerator by chunked vectorized enumeration.
- Collects codewords of small weight and computes their orbits under the
  point-action automorphism subgroup (using closure of point-permutations).

Outputs:
 - bundles/v23_toe_finish/v23/ternary_weight_enumerator.json
 - bundles/v23_toe_finish/v23/ternary_low_weight_orbits.json
"""
from pathlib import Path
import json
import numpy as np
from collections import Counter
import sys, os
# Ensure repo root is importable when running the script directly
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from src.finite_geometry.veldmap import load_triangles, neighborhoods_from_triangles, point_hyperplanes
import igraph as ig
from sympy.combinatorics.permutations import Permutation
from sympy.combinatorics.perm_groups import PermutationGroup

OUT_DIR = Path('bundles/v23_toe_finish/v23')
OUT_DIR.mkdir(parents=True, exist_ok=True)
OUT_JSON = OUT_DIR / 'ternary_weight_enumerator.json'
OUT_ORBITS = OUT_DIR / 'ternary_low_weight_orbits.json'

CHUNK = 200000  # chunk size for vectorized enumeration
LOW_WEIGHT_THRESH = 16


def build_incidence_matrix():
    tri_path = OUT_DIR / 'Q_triangles_with_centers_Z2_S3_fiber6.csv'
    triangles = list(load_triangles(tri_path))
    neighborhoods = neighborhoods_from_triangles(triangles)
    hyperplanes = point_hyperplanes(neighborhoods)
    gens = list(hyperplanes.values())
    points = sorted(set().union(*gens))
    complements = [sorted(set(points) - set(g)) for g in gens]
    v = len(points)
    b = len(complements)
    M = np.zeros((v, b), dtype=int)
    for j, B in enumerate(complements):
        M[B, j] = 1
    return M


def compute_basis_rows_mod3(M):
    # Row space of M over GF(3): compute basis via row-reduction
    A = M.copy() % 3
    A = A.astype(int)
    v, b = A.shape
    used = [False] * v
    basis = []
    for c in range(b):
        pivot = None
        for i in range(v):
            if not used[i] and A[i, c] % 3 != 0:
                pivot = i
                break
        if pivot is None:
            continue
        used[pivot] = True
        pv = A[pivot] % 3
        inv = pow(int(pv[c]), -1, 3)
        pv = (pv * inv) % 3
        basis.append(pv.copy())
        for i in range(v):
            if i != pivot and A[i, c] % 3 != 0:
                A[i, :] = (A[i, :] - A[i, c] * pv) % 3
    B = np.array(basis, dtype=int)
    return B


def enumerate_codewords(basis, chunk=CHUNK, low_thresh=LOW_WEIGHT_THRESH):
    bs, b = basis.shape
    total = 3 ** bs
    hist = Counter()
    low_set = set()
    # precompute powers of 3 for base conversion
    powers = np.array([3 ** i for i in range(bs)], dtype=np.int64)
    rng = range(total)
    # iterate in chunks of indices
    for start in range(0, total, chunk):
        end = min(total, start + chunk)
        idx = np.arange(start, end, dtype=np.int64)
        # compute base-3 digits for idx -> coeffs array shape (n, bs)
        # compute digits via successive division using broadcasting
        coeffs = ((idx[:, None] // powers[None, :]) % 3).astype(np.int8)
        # compute codewords = coeffs @ basis mod 3
        # coeffs: (n, bs), basis: (bs, b)
        code = (coeffs @ basis) % 3
        # compute hamming weight (# nonzero entries)
        weights = (code != 0).sum(axis=1)
        for w in weights:
            hist[int(w)] += 1
        # collect low-weight codewords
        mask = weights <= low_thresh
        if mask.any():
            selected = code[mask]
            for row in selected:
                # store as tuple of 0/1 (nonzero -> 1)
                tup = tuple(int(x != 0) for x in row.tolist())
                low_set.add(tup)
    return hist, low_set


def closure_of_point_permutations():
    # get point-action permutations from igraph automorphism_group bipartition-preserving gens
    tri_path = OUT_DIR / 'Q_triangles_with_centers_Z2_S3_fiber6.csv'
    triangles = list(load_triangles(tri_path))
    neighborhoods = neighborhoods_from_triangles(triangles)
    hyperplanes = point_hyperplanes(neighborhoods)
    gens = list(hyperplanes.values())
    points = sorted(set().union(*gens))
    v = len(points)
    complements = [sorted(set(points) - set(g)) for g in gens]

    M = np.zeros((v, len(complements)), dtype=int)
    for j, B in enumerate(complements):
        M[B, j] = 1
    edges = [(i, v + j) for i in range(v) for j in range(len(complements)) if M[i, j] == 1]
    G = ig.Graph()
    G.add_vertices(2 * v)
    G.add_edges(edges)
    ag = G.automorphism_group()
    preserving = [p for p in ag if all(p[i] < v for i in range(v))]
    # perms as tuples mapping 0..v-1 -> 0..v-1
    perms = [tuple(p[i] for i in range(v)) for p in preserving]
    # close group by BFS
    identity = tuple(range(v))
    group = set([identity])
    frontier = [identity]
    def compose(p, q):
        return tuple(p[i] for i in q)
    while frontier:
        new_frontier = []
        for perm in frontier:
            for s in perms:
                comp = compose(perm, s)
                if comp not in group:
                    group.add(comp)
                    new_frontier.append(comp)
        frontier = new_frontier
        if len(group) > 200000:
            break
    # represent as list of index arrays for fast application
    group_list = [list(p) for p in sorted(group)]
    return group_list


def orbit_representative(tup, group_list):
    v = len(tup)
    best = None
    for perm in group_list:
        # apply perm: new[i] = tup[perm[i]]? we want to permute coordinates by perm
        new = tuple(tup[perm[i]] for i in range(v))
        if best is None or new < best:
            best = new
    return best


def main():
    M = build_incidence_matrix()
    basis = compute_basis_rows_mod3(M)
    bs, b = basis.shape
    print('ternary basis dim', bs, 'length', b)
    hist, low_set = enumerate_codewords(basis, chunk=CHUNK, low_thresh=LOW_WEIGHT_THRESH)
    print('weight histogram keys sample:', sorted(hist.keys())[:10])
    # compute group closure on points
    group_list = closure_of_point_permutations()
    print('group_list size (point action):', len(group_list))
    # compute orbits of low-weight codewords
    rep_map = {}
    for tup in low_set:
        rep = orbit_representative(tup, group_list)
        rep_map.setdefault(rep, 0)
        rep_map[rep] += 1
    # summarize
    out = {
        'basis_dim': int(bs),
        'code_size': 3 ** int(bs),
        'weight_hist': dict(sorted(hist.items())),
        'n_low_weight_codewords': len(low_set),
        'low_weight_orbit_reps_count': len(rep_map),
    }
    with OUT_JSON.open('w', encoding='utf-8') as f:
        json.dump(out, f, indent=2)
    # write orbits map (sample limited)
    sample_orbits = {str(k): v for k, v in list(rep_map.items())[:1000]}
    with OUT_ORBITS.open('w', encoding='utf-8') as f:
        json.dump({'n_orbits': len(rep_map), 'sample_orbits': sample_orbits}, f, indent=2)
    print('Wrote', OUT_JSON, OUT_ORBITS)

if __name__ == '__main__':
    main()
